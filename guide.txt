0) Important to execute from root
    $> cd TFG-backend

1) scrappers to generate catalog
    $> python3 scrappers/generateCatalog.py

2) Calculate text similarity between dataset descriptions 
    local: $> conda run -n similarity-new python3 text-similarity/similarityMatrices.py
    should be: $> python3 text-similarity/similarityMatrices.py

3) Extract dataset columns and calculate profiles
    $> python3 atribute-similarity/columnProfiles.py

4) Categorical rows preprocess + calculate tfidf similarity 
    local: $> conda run -n similarity-new python3 atribute-similarity/similarityCategoricalMatrix.py
    should be: $> python3 atribute-similarity/similarityCategoricalMatrix.py

5) Run API
    $> cd api
    $> uvicorn main:app --reload

*) Update database with new data 
    1) Post data is csv on upload/uploadPackets
    $> python3 upload/upload.py -packet <name>
----------------------------------------------------------------------------

FILE STRUCTURE

|-- scrappers                    => Create an internal catalog of all the available datasets in the web sources
|   |-- scrapperBarcelona.py     => Retrieve information from OpenData Barcelona
|   |-- scrapperHospitalet.py    => Retrieve information from Dades Hospitalet
|   +-- generateCatalog.py       => Unify data from scrappers into catalog database
|-- text-similarity              => Calculate text similarity between dataset descriptions
|   |-- similarityMatrices.py    => Calculate text similarity between dataset descriptions
|   +-- stopwords_ca.py          => stopwords in catalan to be removed from text for similarity comparison
|-- atribute-similarity          => calculate similarity between rows based on profiles 
|   |-- columnProfiles.py        => extract columns from datasets and calculate profiles     
|   +-- similarityCategorical.py => generate similarity tfidf matrix between categorical rows 